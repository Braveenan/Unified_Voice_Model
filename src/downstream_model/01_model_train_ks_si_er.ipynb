{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c96303-7b04-4a83-b3fd-a313ec281e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from dataset.load_embedding import *\n",
    "from dataset.custom_emb_dataloader import *\n",
    "from model.downstream_model import *\n",
    "from trainer.model_trainer import *\n",
    "from evaluator.model_evaluator import *\n",
    "from utils.constant_mapping import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d8dba0-4a2c-42f9-8979-a18b62ef032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(content, filename):\n",
    "    with open(filename, 'a') as f:\n",
    "        print(content, file=f)\n",
    "        \n",
    "def return_current_datetime():\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return_text = f\"Current date and time: {formatted_datetime}\"\n",
    "    return return_text\n",
    "\n",
    "def set_device(device_index=None):\n",
    "    if device_index is not None and torch.cuda.is_available():\n",
    "        num_devices = torch.cuda.device_count()\n",
    "        if num_devices > device_index:\n",
    "            torch.cuda.set_device(device_index)\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"Using GPU {torch.cuda.current_device()}\")\n",
    "            return device\n",
    "        else:\n",
    "            torch.cuda.set_device(0)\n",
    "            device = torch.device(\"cuda:0\")\n",
    "            print(\"Specified GPU index is out of range. Using the first GPU.\")\n",
    "            return device\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available or GPU index is not specified. Using CPU.\")\n",
    "        return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a4d702-ec6e-41f0-9a80-746660a0c9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0\n"
     ]
    }
   ],
   "source": [
    "device = set_device(0)\n",
    "transformer_layer_array = [11, 13, 16]\n",
    "upstream_model_type = \"wavlm_large\"\n",
    "upstream_model_variation = upstream_model_type.split(\"_\")[-1]\n",
    "no_of_encoders = 12 if upstream_model_variation == \"base\" else 24\n",
    "frame_pooling_type = \"mean\"\n",
    "layer_pooling_type = \"mean\"\n",
    "task_type = \"ks_si_er\"\n",
    "data_loading_percentage = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee74e015-0015-4470-8b2e-eb1708be6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tasks_array = task_type.split(\"_\")\n",
    "sub_frame_poolings_array = frame_pooling_type.split(\"_\")\n",
    "transformer_layer_code = \"_\".join(map(str, transformer_layer_array))\n",
    "    \n",
    "label_mapping_speechcommand = LabelMapping.LABEL2INDEX_SPEECHCOMMANDv1.value\n",
    "label_mapping_voxceleb = LabelMapping.LABEL2INDEX_VOXCELEB1.value\n",
    "label_mapping_iemocap = LabelMapping.LABEL2INDEX_IEMOCAP.value\n",
    "    \n",
    "encoder_embed_dim = 768 if upstream_model_variation == \"base\" else 1024\n",
    "model_input_dim = encoder_embed_dim*len(sub_frame_poolings_array)\n",
    "model_embedding_dim_shared = 512\n",
    "    \n",
    "model_embedding_dim_ks = 2000\n",
    "model_embedding_dim_si = 2000\n",
    "model_embedding_dim_er = 1000\n",
    "    \n",
    "model_output_dim_ks = len(label_mapping_speechcommand)\n",
    "model_output_dim_si = len(label_mapping_voxceleb)\n",
    "model_output_dim_er = len(label_mapping_iemocap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c006cf9e-76d6-4a85-a443-19f1e48b8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "num_epochs = 5\n",
    "learning_rate = 2.5e-3\n",
    "weight_decay = 5e-8\n",
    "saved_checkpoint_count = 1\n",
    "patience = 1\n",
    "factor = 0.5\n",
    "    \n",
    "dropout_prob_shared = 0.7\n",
    "dropout_prob_ks = 0.2\n",
    "dropout_prob_si = 0.6\n",
    "dropout_prob_er = 0.3\n",
    "\n",
    "l1_lambda = 0\n",
    "l2_lambda = 1e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab217674-3b00-4862-86aa-6514e52b08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/userdirs/braveenan2/voice_dataset\"\n",
    "root_speechcommand = os.path.join(root_path, \"SpeechCommand\")\n",
    "root_voxceleb = os.path.join(root_path, \"VoxCeleb\")\n",
    "root_iemocap = os.path.join(root_path, \"IEMOCAP\")\n",
    "    \n",
    "root_emb_path = f\"/userdirs/braveenan2/embedding/{upstream_model_type}/{frame_pooling_type}\"\n",
    "root_emb_speechcommand = os.path.join(root_emb_path, \"SpeechCommand\")\n",
    "root_emb_voxceleb = os.path.join(root_emb_path, \"VoxCeleb\")\n",
    "root_emb_iemocap = os.path.join(root_emb_path, \"IEMOCAP\")\n",
    "    \n",
    "current_timestamp = str(int(time.time()))\n",
    "result_folder_path = f\"result/{upstream_model_type}/{transformer_layer_code}/{current_timestamp}\"\n",
    "checkpoint_folder_path = f\"checkpoint/{upstream_model_type}/{transformer_layer_code}/{current_timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1cc84db-7d34-4612-bbd6-849d9328a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_path(upstream_model_type, task_type, folder_path, file_format):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_name = f\"{upstream_model_type}_{task_type}{file_format}\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    return file_path\n",
    "    \n",
    "result_text_path_all = create_file_path(upstream_model_type, task_type, result_folder_path, \".txt\")\n",
    "result_plot_path_all = create_file_path(upstream_model_type, task_type, result_folder_path, \".png\")\n",
    "result_text_path_task1 = create_file_path(upstream_model_type, sub_tasks_array[0], result_folder_path, \".txt\")\n",
    "result_plot_path_task1 = create_file_path(upstream_model_type, sub_tasks_array[0], result_folder_path, \".png\")\n",
    "result_text_path_task2 = create_file_path(upstream_model_type, sub_tasks_array[1], result_folder_path, \".txt\")\n",
    "result_plot_path_task2 = create_file_path(upstream_model_type, sub_tasks_array[1], result_folder_path, \".png\")\n",
    "result_text_path_task3 = create_file_path(upstream_model_type, sub_tasks_array[2], result_folder_path, \".txt\")\n",
    "result_plot_path_task3 = create_file_path(upstream_model_type, sub_tasks_array[2], result_folder_path, \".png\")\n",
    "model_checkpoint_path = create_file_path(upstream_model_type, task_type, checkpoint_folder_path, \".pth\")\n",
    "data_count_path = create_file_path(upstream_model_type, \"data_count\", result_folder_path, \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceda8d03-cd0f-4215-9eb5-492b86605c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dimensions_dict = {\n",
    "    \"ks\": (model_output_dim_ks, model_embedding_dim_ks),\n",
    "    \"si\": (model_output_dim_si, model_embedding_dim_si),\n",
    "    \"er\": (model_output_dim_er, model_embedding_dim_er)\n",
    "}\n",
    "    \n",
    "dataset_dict = {\n",
    "    \"ks\": \"speechcommand\",\n",
    "    \"si\": \"voxceleb\",\n",
    "    \"er\": \"iemocap\"\n",
    "}\n",
    "    \n",
    "label_dict = {\n",
    "    \"ks\": \"content\",\n",
    "    \"si\": \"speaker\",\n",
    "    \"er\": \"emotion\"\n",
    "}\n",
    "    \n",
    "dropout_prob_dict = {\n",
    "    \"ks\": dropout_prob_ks,\n",
    "    \"si\": dropout_prob_si,\n",
    "    \"er\": dropout_prob_er\n",
    "}\n",
    "    \n",
    "dataset_root_dict = {\n",
    "    \"speechcommand\": root_speechcommand,\n",
    "    \"voxceleb\": root_voxceleb,\n",
    "    \"iemocap\": root_iemocap\n",
    "}\n",
    "    \n",
    "embedding_root_dict = {\n",
    "    \"speechcommand\": root_emb_speechcommand,\n",
    "    \"voxceleb\": root_emb_voxceleb,\n",
    "    \"iemocap\": root_emb_iemocap\n",
    "}\n",
    "    \n",
    "label_mapping_dict = {\n",
    "    \"speechcommand\": label_mapping_speechcommand,\n",
    "    \"voxceleb\": label_mapping_voxceleb,\n",
    "    \"iemocap\": label_mapping_iemocap\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e04653a-e363-419d-87d9-02cf7ba7b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dim_array = []\n",
    "model_embedding_dim_array = []\n",
    "dataset_array = []\n",
    "label_array = []\n",
    "dropout_prob_array = []\n",
    "    \n",
    "for task in sub_tasks_array:\n",
    "    output_dim, embedding_dim = task_dimensions_dict[task]\n",
    "    model_output_dim_array.append(output_dim)\n",
    "    model_embedding_dim_array.append(embedding_dim)\n",
    "        \n",
    "    dataset_name = dataset_dict[task]\n",
    "    dataset_array.append(dataset_name)\n",
    "        \n",
    "    label_name = label_dict[task]\n",
    "    label_array.append(label_name)\n",
    "        \n",
    "    dropout_prob_name = dropout_prob_dict[task]\n",
    "    dropout_prob_array.append(dropout_prob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f105b6c-3fa2-46e4-91df-d6625be5836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2024-03-05 11:22:03\n"
     ]
    }
   ],
   "source": [
    "current_datetime = return_current_datetime()\n",
    "print(current_datetime)\n",
    "save_to_txt(current_datetime, result_text_path_all)\n",
    "save_to_txt(current_datetime, result_text_path_task1)\n",
    "save_to_txt(current_datetime, result_text_path_task2)\n",
    "save_to_txt(current_datetime, result_text_path_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4368a935-7225-4922-b34e-2675c3dc3e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks name: Keyword Spotting, Speaker Identification and Emotion Recognition\n"
     ]
    }
   ],
   "source": [
    "task1_name = KeywordMapping.get_task_name(sub_tasks_array[0])\n",
    "task1_text = f\"Task1 name: {task1_name}\"\n",
    "save_to_txt(task1_text, result_text_path_task1)\n",
    "task2_name = KeywordMapping.get_task_name(sub_tasks_array[1])\n",
    "task2_text = f\"Task2 name: {task2_name}\"\n",
    "save_to_txt(task2_text, result_text_path_task2)\n",
    "task3_name = KeywordMapping.get_task_name(sub_tasks_array[2])\n",
    "task3_text = f\"Task3 name: {task3_name}\"\n",
    "save_to_txt(task3_text, result_text_path_task3)\n",
    "tasks_name = f\"{task1_name}, {task2_name} and {task3_name}\"\n",
    "tasks_text = f\"Tasks name: {tasks_name}\"\n",
    "print(tasks_text)\n",
    "save_to_txt(tasks_text, result_text_path_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24195c2e-3a30-4ea5-873b-5acc7f3cb657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream model type: wavlm_large\n",
      "Transformer layers: 11,13,16\n",
      "Layer pooling type: mean\n"
     ]
    }
   ],
   "source": [
    "upstreammodel_text = f\"Upstream model type: {upstream_model_type}\"\n",
    "print(upstreammodel_text)\n",
    "save_to_txt(upstreammodel_text, result_text_path_all)\n",
    "save_to_txt(upstreammodel_text, result_text_path_task1)\n",
    "save_to_txt(upstreammodel_text, result_text_path_task2)\n",
    "save_to_txt(upstreammodel_text, result_text_path_task3)\n",
    "    \n",
    "transformer_text = f\"Transformer layers: {','.join(map(str, transformer_layer_array))}\"\n",
    "print(transformer_text)\n",
    "save_to_txt(transformer_text, result_text_path_all)\n",
    "save_to_txt(transformer_text, result_text_path_task1)\n",
    "save_to_txt(transformer_text, result_text_path_task2)\n",
    "save_to_txt(transformer_text, result_text_path_task3)\n",
    "    \n",
    "layer_pooling_text = f\"Layer pooling type: {layer_pooling_type}\"\n",
    "print(layer_pooling_text)\n",
    "save_to_txt(layer_pooling_text, result_text_path_all)\n",
    "save_to_txt(layer_pooling_text, result_text_path_task1)\n",
    "save_to_txt(layer_pooling_text, result_text_path_task2)\n",
    "save_to_txt(layer_pooling_text, result_text_path_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d1de172-af43-4e7b-8e53-41986b1b831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: speechcommand, voxceleb and iemocap\n",
      "No of training data samples: 64625 \n",
      "No of validation data samples: 4752\n"
     ]
    }
   ],
   "source": [
    "loader = LoadEmbedding(\n",
    "    dataset_root_dict=dataset_root_dict,\n",
    "    embedding_root_dict=embedding_root_dict,\n",
    "    label_mapping_dict=label_mapping_dict,\n",
    "    frame_pooling_type = frame_pooling_type,\n",
    "    upstream_model_type=upstream_model_type,\n",
    "    transformer_layer_array=transformer_layer_array,\n",
    "    device=device\n",
    ")\n",
    "    \n",
    "dataset_text = f\"Dataset name: {dataset_array[0]}, {dataset_array[1]} and {dataset_array[2]}\"\n",
    "print(dataset_text)\n",
    "save_to_txt(dataset_text, result_text_path_all)\n",
    "save_to_txt(dataset_text, result_text_path_task1)\n",
    "save_to_txt(dataset_text, result_text_path_task2)\n",
    "save_to_txt(dataset_text, result_text_path_task3)\n",
    "    \n",
    "dataset_loading_code = \"_\".join(dataset_array)\n",
    "training_data, validation_data, testing_data = loader.load_embedding(dataset_loading_code, data_loading_percentage)\n",
    "dataset_length_text = f\"No of training data samples: {len(training_data)} \\nNo of validation data samples: {len(validation_data)}\"\n",
    "print(dataset_length_text)\n",
    "save_to_txt(dataset_length_text, result_text_path_all)\n",
    "save_to_txt(dataset_length_text, result_text_path_task1)\n",
    "save_to_txt(dataset_length_text, result_text_path_task2)\n",
    "save_to_txt(dataset_length_text, result_text_path_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e4911f-6222-4393-8dae-731f346f64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = CustomEmbDataLoader(training_data, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "val_dataloader = CustomEmbDataLoader(validation_data, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b05d2361-bd2a-483f-815f-97edb45d5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DownstreamTripleTaskModel(\n",
      "  (projector_layer): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout_shared): Dropout(p=0.7, inplace=False)\n",
      "  (hidden_layer1): Linear(in_features=512, out_features=2000, bias=True)\n",
      "  (dropout_task1): Dropout(p=0.2, inplace=False)\n",
      "  (hidden_layer2): Linear(in_features=512, out_features=2000, bias=True)\n",
      "  (dropout_task2): Dropout(p=0.6, inplace=False)\n",
      "  (hidden_layer3): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  (dropout_task3): Dropout(p=0.3, inplace=False)\n",
      "  (classifier_task1): Linear(in_features=2000, out_features=12, bias=True)\n",
      "  (classifier_task2): Linear(in_features=2000, out_features=1251, bias=True)\n",
      "  (classifier_task3): Linear(in_features=1000, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DownstreamTripleTaskModel(model_input_dim, model_output_dim_array, model_embedding_dim_shared, model_embedding_dim_array, layer_pooling_type, dropout_prob_shared, dropout_prob_array)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efaf482e-24a1-49a1-beeb-0bfe16e18eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_parameters = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"weight_decay\": weight_decay\n",
    "}\n",
    "scheduler_parameters = {\n",
    "    \"patience\": patience, \n",
    "    \"factor\": factor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7b536b-d80c-4d60-92ac-baf956d87d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss all: 7.2872, Train Accuracy all: 0.1752, Test Loss all: 2.2282, Test Accuracy all: 0.5107\n",
      "Epoch 1/5 - Train Loss content: 9.9249, Train Accuracy content: 0.5299, Test Loss content: 0.5056, Test Accuracy content: 0.8897\n",
      "Epoch 1/5 - Train Loss speaker: 7.3173, Train Accuracy speaker: 0.0355, Test Loss speaker: 4.7924, Test Accuracy speaker: 0.1327\n",
      "Epoch 1/5 - Train Loss emotion: 4.6115, Train Accuracy emotion: 0.4479, Test Loss emotion: 1.3781, Test Accuracy emotion: 0.5337\n",
      "Epoch 2/5 - Train Loss all: 1.8904, Train Accuracy all: 0.4183, Test Loss all: 1.2675, Test Accuracy all: 0.6790\n",
      "Epoch 2/5 - Train Loss content: 0.3991, Train Accuracy content: 0.9200, Test Loss content: 0.1294, Test Accuracy content: 0.9704\n",
      "Epoch 2/5 - Train Loss speaker: 4.0450, Train Accuracy speaker: 0.2274, Test Loss speaker: 2.9800, Test Accuracy speaker: 0.3850\n",
      "Epoch 2/5 - Train Loss emotion: 1.2180, Train Accuracy emotion: 0.5924, Test Loss emotion: 0.6833, Test Accuracy emotion: 0.7580\n",
      "Epoch 3/5 - Train Loss all: 1.2405, Train Accuracy all: 0.5817, Test Loss all: 0.9260, Test Accuracy all: 0.7649\n",
      "Epoch 3/5 - Train Loss content: 0.2101, Train Accuracy content: 0.9537, Test Loss content: 0.1222, Test Accuracy content: 0.9724\n",
      "Epoch 3/5 - Train Loss speaker: 2.5423, Train Accuracy speaker: 0.4426, Test Loss speaker: 1.9411, Test Accuracy speaker: 0.5610\n",
      "Epoch 3/5 - Train Loss emotion: 0.9589, Train Accuracy emotion: 0.6387, Test Loss emotion: 0.7037, Test Accuracy emotion: 0.7484\n",
      "Epoch 4/5 - Train Loss all: 1.0079, Train Accuracy all: 0.6727, Test Loss all: 0.7352, Test Accuracy all: 0.8210\n",
      "Epoch 4/5 - Train Loss content: 0.1946, Train Accuracy content: 0.9582, Test Loss content: 0.1274, Test Accuracy content: 0.9730\n",
      "Epoch 4/5 - Train Loss speaker: 1.8826, Train Accuracy speaker: 0.5682, Test Loss speaker: 1.3890, Test Accuracy speaker: 0.6751\n",
      "Epoch 4/5 - Train Loss emotion: 0.9353, Train Accuracy emotion: 0.6467, Test Loss emotion: 0.6775, Test Accuracy emotion: 0.7516\n",
      "Epoch 5/5 - Train Loss all: 0.9267, Train Accuracy all: 0.7147, Test Loss all: 0.7178, Test Accuracy all: 0.8345\n",
      "Epoch 5/5 - Train Loss content: 0.1892, Train Accuracy content: 0.9575, Test Loss content: 0.0923, Test Accuracy content: 0.9781\n",
      "Epoch 5/5 - Train Loss speaker: 1.6419, Train Accuracy speaker: 0.6270, Test Loss speaker: 1.2749, Test Accuracy speaker: 0.7011\n",
      "Epoch 5/5 - Train Loss emotion: 0.9370, Train Accuracy emotion: 0.6510, Test Loss emotion: 0.7737, Test Accuracy emotion: 0.7205\n",
      "Best model accuracy - all: 0.83447265625\n",
      "Best model accuracy - content: 0.9780724120346762\n",
      "Best model accuracy - speaker: 0.7011144883485309\n",
      "Best model accuracy - emotion: 0.7204968944099379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = TripleTasksModelTrainer(model, optimizer_parameters, scheduler_parameters, device, label_array, num_epochs, saved_checkpoint_count, l1_lambda, l2_lambda)\n",
    "trainer.train_dataloader = train_dataloader\n",
    "trainer.test_dataloader = val_dataloader\n",
    "trainer.data_count_path = data_count_path\n",
    "trainer.result_text_path_all = result_text_path_all\n",
    "trainer.result_plot_path_all = result_plot_path_all\n",
    "trainer.result_text_path_task1 = result_text_path_task1\n",
    "trainer.result_plot_path_task1 = result_plot_path_task1\n",
    "trainer.result_text_path_task2 = result_text_path_task2\n",
    "trainer.result_plot_path_task2 = result_plot_path_task2\n",
    "trainer.result_text_path_task3 = result_text_path_task3\n",
    "trainer.result_plot_path_task3 = result_plot_path_task3\n",
    "trainer.model_checkpoint_path = model_checkpoint_path\n",
    "trainer.plot_title_all = tasks_name\n",
    "trainer.plot_title_task1 = task1_name\n",
    "trainer.plot_title_task2 = task2_name\n",
    "trainer.plot_title_task3 = task3_name\n",
    "    \n",
    "# Train the model\n",
    "trainer.train()\n",
    "    \n",
    "# Plot metrics separately when needed\n",
    "trainer.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a751003-f87c-40ed-923c-4037980f9fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2024-03-05 11:23:09\n"
     ]
    }
   ],
   "source": [
    "current_datetime = return_current_datetime()\n",
    "print(current_datetime)\n",
    "save_to_txt(current_datetime, result_text_path_all)\n",
    "save_to_txt(current_datetime, result_text_path_task1)\n",
    "save_to_txt(current_datetime, result_text_path_task2)\n",
    "save_to_txt(current_datetime, result_text_path_task3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b72776-929b-4f6c-9453-625b347b6e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint/wavlm_large/11_13_16/1709617922/wavlm_large_ks_si_er_best.pth\n"
     ]
    }
   ],
   "source": [
    "best_checkpoint_path = model_checkpoint_path.replace(\".pth\", \"_best.pth\")\n",
    "print(best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efef1199-8287-4e10-8ac1-cc911262d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_file_name = best_checkpoint_path.replace(\".pth\", \"\")\n",
    "best_file_name = best_file_name.replace(\"checkpoint/\", \"result/\")\n",
    "best_text_path = f\"{best_file_name}_eval.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59a34d3d-1562-448e-87a9-093cd97646ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2024-03-05 11:23:09\n"
     ]
    }
   ],
   "source": [
    "current_datetime = return_current_datetime()\n",
    "print(current_datetime)\n",
    "save_to_txt(current_datetime, best_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8800c66c-2de0-415e-b69a-ccde28e4d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_txt(tasks_text, best_text_path)\n",
    "save_to_txt(upstreammodel_text, best_text_path)\n",
    "save_to_txt(transformer_text, best_text_path)\n",
    "save_to_txt(layer_pooling_text, best_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad375215-3adf-4117-81dd-2920d9cc3c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: speechcommand, voxceleb and iemocap\n",
      "No of testing data samples: 5213\n"
     ]
    }
   ],
   "source": [
    "print(dataset_text)\n",
    "save_to_txt(dataset_text, best_text_path)\n",
    "\n",
    "dataset_length_text = f\"No of testing data samples: {len(testing_data)}\"\n",
    "print(dataset_length_text)\n",
    "save_to_txt(dataset_length_text, best_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5600482-e969-4ef2-a9e3-1931e1ee33f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss all: 0.2367, Accuracy all: 0.8306\n",
      "Loss content: 0.0904, Accuracy content: 0.9811\n",
      "Loss speaker: 1.2116, Accuracy speaker: 0.7171\n",
      "Loss emotion: 0.8879, Accuracy emotion: 0.6630\n"
     ]
    }
   ],
   "source": [
    "evaluator = TripleTaskModelEvaluator(model, best_checkpoint_path, testing_data, device, label_array)\n",
    "evaluator.get_loss_and_accuracy(best_text_path)\n",
    "evaluator.get_labels_and_predictions(best_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd961d0f-4fc0-4e60-be99-c83993c8a1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time: 2024-03-05 11:23:22\n"
     ]
    }
   ],
   "source": [
    "current_datetime = return_current_datetime()\n",
    "print(current_datetime)\n",
    "save_to_txt(current_datetime, best_text_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
